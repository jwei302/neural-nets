{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6ac6fb",
   "metadata": {},
   "source": [
    "# Building a Generatively Pretrained Transformer (GPT)\n",
    "\n",
    "Following Andrej Karpathy's [tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1044s) and Attention Is All You Need [paper](https://arxiv.org/pdf/1706.03762). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dddc35",
   "metadata": {},
   "source": [
    "## Data Processing, Encoder Decoder, Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3b65af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n",
      "len(text)=1125396\n"
     ]
    }
   ],
   "source": [
    "# get training text. \n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:300])\n",
    "print(f'{len(text)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea55fb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab_size=65\n"
     ]
    }
   ],
   "source": [
    "# all the unique chracters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(f'{vocab_size=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da090e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# encode text\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "# decode list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "print(decode(encode(\"hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e661ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1125396]) torch.int64\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "# encode text dataset and store as a tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(decode(data[:300].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcc199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and validation sets\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e86c5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: xb.shape=torch.Size([4, 8])\n",
      "Target Shape: yb.shape=torch.Size([4, 8])\n",
      "Inputs: xb=tensor([[58,  8,  0, 16, 53,  1, 63, 53],\n",
      "        [46, 43, 39, 56,  1, 51, 43,  6],\n",
      "        [ 1, 51, 63,  1, 51, 39, 57, 58],\n",
      "        [ 1, 39, 57,  1, 58, 46, 53, 59]])\n",
      "Targets: yb=tensor([[ 8,  0, 16, 53,  1, 63, 53, 59],\n",
      "        [43, 39, 56,  1, 51, 43,  6,  1],\n",
      "        [51, 63,  1, 51, 39, 57, 58, 43],\n",
      "        [39, 57,  1, 58, 46, 53, 59,  1]])\n",
      "===============\n",
      "When input is [58] the target is 8\n",
      "When input is [58, 8] the target is 0\n",
      "When input is [58, 8, 0] the target is 16\n",
      "When input is [58, 8, 0, 16] the target is 53\n",
      "When input is [58, 8, 0, 16, 53] the target is 1\n",
      "When input is [58, 8, 0, 16, 53, 1] the target is 63\n",
      "When input is [58, 8, 0, 16, 53, 1, 63] the target is 53\n",
      "When input is [58, 8, 0, 16, 53, 1, 63, 53] the target is 59\n",
      "When input is [46] the target is 43\n",
      "When input is [46, 43] the target is 39\n",
      "When input is [46, 43, 39] the target is 56\n",
      "When input is [46, 43, 39, 56] the target is 1\n",
      "When input is [46, 43, 39, 56, 1] the target is 51\n",
      "When input is [46, 43, 39, 56, 1, 51] the target is 43\n",
      "When input is [46, 43, 39, 56, 1, 51, 43] the target is 6\n",
      "When input is [46, 43, 39, 56, 1, 51, 43, 6] the target is 1\n",
      "When input is [1] the target is 51\n",
      "When input is [1, 51] the target is 63\n",
      "When input is [1, 51, 63] the target is 1\n",
      "When input is [1, 51, 63, 1] the target is 51\n",
      "When input is [1, 51, 63, 1, 51] the target is 39\n",
      "When input is [1, 51, 63, 1, 51, 39] the target is 57\n",
      "When input is [1, 51, 63, 1, 51, 39, 57] the target is 58\n",
      "When input is [1, 51, 63, 1, 51, 39, 57, 58] the target is 43\n",
      "When input is [1] the target is 39\n",
      "When input is [1, 39] the target is 57\n",
      "When input is [1, 39, 57] the target is 1\n",
      "When input is [1, 39, 57, 1] the target is 58\n",
      "When input is [1, 39, 57, 1, 58] the target is 46\n",
      "When input is [1, 39, 57, 1, 58, 46] the target is 53\n",
      "When input is [1, 39, 57, 1, 58, 46, 53] the target is 59\n",
      "When input is [1, 39, 57, 1, 58, 46, 53, 59] the target is 1\n"
     ]
    }
   ],
   "source": [
    "# we train the transformer on random chunks of the text. \n",
    "# this chunk is known as the context length\n",
    "# we do this in parallel in different batches\n",
    "\n",
    "batch_size = 4 # sequences processed in parallel\n",
    "context_size = 8 # maximum context length\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(f'Input Shape: {xb.shape=}')\n",
    "print(f'Target Shape: {yb.shape=}')\n",
    "print(f'Inputs: {xb=}')\n",
    "print(f'Targets: {yb=}')\n",
    "\n",
    "print('===============')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(context_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'When input is {context.tolist()} the target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948ed17",
   "metadata": {},
   "source": [
    "## Bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aacb131",
   "metadata": {},
   "source": [
    "We implement a simple bigram language model as a baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b6ca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-8.9222e-01, -4.5212e-01,  5.3472e-01, -5.2330e-01, -1.2478e+00,\n",
      "         2.9198e-01, -2.6053e-02, -1.2701e-01, -1.9188e+00, -9.1911e-03,\n",
      "         7.3610e-01, -7.9553e-01, -7.4883e-01, -2.4543e-02, -1.0343e+00,\n",
      "         1.4419e-01,  7.0602e-01, -2.3258e-01,  1.7613e-01,  6.4299e-01,\n",
      "        -1.1068e+00, -5.7910e-01, -1.4749e+00, -1.4360e+00, -2.3925e-01,\n",
      "         6.4835e-01,  2.0337e+00, -1.3335e+00,  3.6340e-01,  9.9667e-01,\n",
      "        -5.9896e-01,  2.7289e-01,  5.0060e-01, -5.3503e-01,  6.5793e-01,\n",
      "        -1.4281e+00, -1.0557e+00, -3.2065e-01,  5.5331e-01, -2.2441e-01,\n",
      "         4.9368e-03, -8.5709e-01, -6.2144e-01, -2.9752e+00, -1.0889e+00,\n",
      "        -1.0538e+00,  1.8081e+00,  7.5744e-01,  1.4311e+00,  1.4998e+00,\n",
      "         6.5244e-01, -2.6761e-02, -1.3566e+00, -1.6688e-03,  1.4140e+00,\n",
      "         1.0523e+00, -1.4516e+00,  9.4369e-01,  5.5688e-02, -1.8889e+00,\n",
      "        -5.4709e-01,  1.7760e+00,  8.6275e-01, -5.1063e-01, -5.2403e-01],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor(2.8603, grad_fn=<DivBackward1>)\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1],\n",
       "        [2, 3, 2, 3],\n",
       "        [4, 5, 4, 5],\n",
       "        [6, 7, 6, 7],\n",
       "        [8, 9, 8, 9]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# embedding table, essentially a weight matrix\n",
    "W = nn.Embedding(vocab_size, vocab_size)\n",
    "print(W(torch.tensor(1)))\n",
    "\n",
    "# cross entropy loss\n",
    "# using nn.CrossEntropyLoss() as opposed F.cross_entropy() as latter is a functional\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(1, 4, requires_grad=True)\n",
    "target = torch.randn(1, 4)\n",
    "print(loss(input, target))\n",
    "\n",
    "# torch.cat()\n",
    "a = torch.arange(10).view(5, 2)\n",
    "b = torch.arange(10).view(5, 2)\n",
    "print(a)\n",
    "print(b)\n",
    "torch.cat((a, b), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff85023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (B=batch_size, T=time, C=vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            # idx and targets are both (B, T) tesnors of integers, but we need them to be (B*T)\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, new_token_length):\n",
    "        # idx is (B, T) array of indices in current context\n",
    "        for _ in range(new_token_length):\n",
    "            # get predictions \n",
    "            logits, loss = self(idx)\n",
    "            # we only want the last time step\n",
    "            logits = logits[:, -1, :]  #(B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2290d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape=torch.Size([32, 65])\n",
      "loss.item()=4.831140041351318\n"
     ]
    }
   ],
   "source": [
    "# initiating simple bigram model with batched inputs and outputs\n",
    "model = BigramLM(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(f'{logits.shape=}')\n",
    "print(f'{loss.item()=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79055958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QIhSXXx$SxEerU.a' vEkFM,B OuW\n",
      "v&QUQk-CRqTGrm;hk,qT\n",
      "i-Qk.ud-WKHFJoiIdYkt'pM;F:bZzpLSHF&WfaabD&gjAYGrq\n"
     ]
    }
   ],
   "source": [
    "# running inference on bigram model\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(idx, new_token_length=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b3c411",
   "metadata": {},
   "source": [
    "Obviously, the output is gibberish. The model is completely random weights. Let's train it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e38f90a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "626d46eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2513909339904785\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "# simple training loop\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    #evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # zero out gradients to avoid gradient accumulation\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backwards pass the loss \n",
    "    loss.backward()\n",
    "\n",
    "    # gradient descend. \n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dca45adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "de'derselishar, s d hatonth ceerind.\n",
      "Dif!\n",
      "I ge ce: alend itceamaroancadve ach s\n",
      "Fo Hoio ang IDUCaime ththat hit u nins vese he ar he mythe.\n",
      "\n",
      "MENCENus adf ath oushot:\n",
      "D:\n",
      "\n",
      "Antyay od thirchiro. that hile o wis CEr lol--S: to w d miod thth wif bey y hare apowisu ssteeemellow t er w's, weered'ere remers \n"
     ]
    }
   ],
   "source": [
    "# running inference on bigram model\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(idx, new_token_length=300)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ccff2d",
   "metadata": {},
   "source": [
    "Getting something much more reasonable after training loop! However, this is just a naive bigram model, where a token's context is just the token before it. Let's level up to **attention** and **transformers**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f553a0",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd285e1",
   "metadata": {},
   "source": [
    "**Toy Example**: we want the current token to have all the previous tokens as context. A naive implementation is to average all previous token values and use that as context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ae4d727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy Example\n",
    "\n",
    "# we want the current token to have all the previous tokens as context\n",
    "# a naive implementation is to average all previous token values and use that as context \n",
    "\n",
    "torch.manual_seed(1)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# bow = bag of words, meaning an unordered collection of word frequencies\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B): \n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] #(t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf8f74",
   "metadata": {},
   "source": [
    "The implementation above is highly inefficient. We utilize matrix multiplication to parallelize the process. Specifically, we use lower triangular matrix to calculate running sums to efficiently extract the averages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d1fa8cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=tensor([[5., 9.],\n",
      "        [4., 8.],\n",
      "        [3., 3.]])\n",
      "c=tensor([[5.0000, 9.0000],\n",
      "        [4.5000, 8.5000],\n",
      "        [4.0000, 6.6667]])\n"
     ]
    }
   ],
   "source": [
    "# take lower triangle of square matrix\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a /= torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(f'{a=}')\n",
    "print(f'{b=}')\n",
    "print(f'{c=}')\n",
    "\n",
    "# now we get running average of the columns of b in c. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d8977",
   "metadata": {},
   "source": [
    "Now, apply it onto our running average calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5975e38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights=tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "weights=tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "print(f'{weights=}')\n",
    "weights /= weights.sum(dim=1, keepdim=True)\n",
    "print(f'{weights=}')\n",
    "xbow2 = weights @ x\n",
    "# allclose checks if the matrices are similar. \n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64610185",
   "metadata": {},
   "source": [
    "A third method, using Softmax, also produces the desired result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d8d41391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights=tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "weights=tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril==0, float('-inf'))\n",
    "print(f'{weights=}')\n",
    "weights = F.softmax(weights, dim=1)\n",
    "print(f'{weights=}')\n",
    "xbow3 = weights @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb03cbdb",
   "metadata": {},
   "source": [
    "We use this third method in self-attention because softmax presents a nice intuitive to understand the natural affinity between elements. Specifically, the weights matrix is an affinity between the current token and previous tokens, with larger values representing more **attention** needed between that pair. \n",
    "\n",
    "Importantly, we also clamp the elements so that the current token can only talk to the tokens of the past, and can not talk to the tokens of the future. \n",
    "\n",
    "Below, we perform **self-attention**. We introduce three new concepts\n",
    "- Key: a matrix representing the *value* of the previous tokens.\n",
    "- Query: a matrix representing what the current token is looking for.\n",
    "- Value: a matrix representing the information of the current token. \n",
    "\n",
    "These are all implemented by simple linear layers. Then, a simple *dot product* between the two represents the affinity between the query of the current token and the keys of the previous tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "748c9ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape=torch.Size([2, 3, 4])\n",
      "a.shape=torch.Size([3, 2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[ 4,  5,  6,  7],\n",
       "         [16, 17, 18, 19]],\n",
       "\n",
       "        [[ 8,  9, 10, 11],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# playing around with transpose\n",
    "\n",
    "a = torch.arange(24).view(2, 3, 4)\n",
    "print(f'{a.shape=}')\n",
    "a = a.transpose(0, 1)\n",
    "print(f'{a.shape=}')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bfa96b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0]=tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.1865e-01, 8.8135e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.9568e-01, 4.2274e-01, 1.8159e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.3539e-01, 5.5080e-01, 1.4647e-01, 6.7341e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.2729e-01, 2.4094e-02, 2.2063e-01, 2.9514e-01, 3.3284e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.2596e-01, 6.3780e-02, 6.7478e-02, 2.8705e-01, 4.0217e-01, 5.3562e-02,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.1200e-02, 5.7241e-04, 3.3019e-01, 4.5086e-02, 2.1366e-02, 5.0661e-02,\n",
      "         5.3092e-01, 0.0000e+00],\n",
      "        [4.2966e-01, 9.4396e-03, 1.4951e-01, 6.9958e-02, 2.1870e-01, 7.7199e-02,\n",
      "         1.4871e-02, 3.0660e-02]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# self attention\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# single head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, C) --> (B, T, 16)\n",
    "q = query(x) # (B, T, C) --> (B, T, 16)\n",
    "weights = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "\n",
    "# do the weighted average\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "weights = weights.masked_fill(tril==0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "# now we have weights for each batch of data. \n",
    "v = value(x)\n",
    "out = weights @ v\n",
    "\n",
    "print(f'{weights[0]=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127b743",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. Attention is a *communication mechanism*. It's like nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them with data-dependent weights. \n",
    "2. There is no notion of space. Unlike convolutions which in essence have some spatial encoding. This is why we need postionally encoded tokens, or a positional embedding vector.\n",
    "3. Examples across batch dimension is processed completely independently and never \"communicate\" each other. This allows attention to be such an efficient mechanism because it parallels the mechanism. \n",
    "4. In an \"encoder\" attention block, you remove the `torch.tril` line, allowing the tokens to freely communicate with each other. The implementation above is a \"decoder\" implementation because it has triangular masking, and is usually used in autoregressive settings so that tokens in the future can not talk to the current token. \n",
    "5. **Self-attention** just means that the keys and values are produced from the same source. **Cross-attention** means that you are attending to a separate source of tokens to pool information from. \n",
    "6. **Scaled-Attention** divides the `weight` matrix by `1/sqrt(head_size)`. This makes so when input $Q, K$ are unit variance, `weight` will be unit variance too and softmax will not saturate too much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "df4c857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k.var()=tensor(1.0047), q.var()=tensor(0.9612), weights.var()=tensor(16.4341)\n",
      "k.var()=tensor(1.0047), q.var()=tensor(0.9612), weights.var()=tensor(1.0271)\n",
      "tensor([0.1621, 0.1980, 0.2419, 0.1792, 0.2188])\n",
      "tensor([0.0117, 0.0861, 0.6364, 0.0317, 0.2341])\n",
      "tensor([4.2482e-18, 2.0611e-09, 9.9995e-01, 9.3572e-14, 4.5398e-05])\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of scaled-attention\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "weights = q @ k.transpose(-2, -1)\n",
    "print(f'{k.var()=}, {q.var()=}, {weights.var()=}')\n",
    "weights = q @ k.transpose(-2, -1) * head_size ** -0.5\n",
    "print(f'{k.var()=}, {q.var()=}, {weights.var()=}')\n",
    "# scales back down to unit variance \n",
    "\n",
    "# the reason we want to scale this down is because \n",
    "print(torch.softmax(torch.tensor([0.1, 0.3, 0.5, 0.2, 0.4]), dim=-1))\n",
    "print(torch.softmax(torch.tensor([0.1, 0.3, 0.5, 0.2, 0.4])*10, dim=-1))\n",
    "print(torch.softmax(torch.tensor([0.1, 0.3, 0.5, 0.2, 0.4])*100, dim=-1))\n",
    "# the larger values get larger, and in the limit this becomes a one-hot embedding.https://job-boards.greenhouse.io/lilasciences/jobs/4031379009 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
